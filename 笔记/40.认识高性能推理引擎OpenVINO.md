# 认识高性能推理引擎OpenVINO

## 从 0 到 1 的部署、调优与量化



### 学习目标

- 知道 OpenVINO 是什么、能做什么（在 CPU 上为什么快）
- 能用 OMZ 下载现成模型并跑通推理与 benchmark
- 掌握 CPU 上三件事：性能 hint、并发（NUM_STREAMS）、编译缓存
- 会用 NNCF 做 PTQ（后训练量化）并验证性能收益


#### 1. 为什么选 OpenVINO

- 接入容易：对于Python工程师，pip 安装一套 Runtime + 工具链（benchmark_app、OMZ 工具）
- 接口统一：单一 API，原生支持 CPU 高效推理（oneDNN、图优化、算子融合）
- 交付简单：可离线、可启用编译缓存，跨平台（Win/Linux/macOS x86）
- 性能开关简单：PERFORMANCE_HINT、NUM_STREAMS、CACHE_DIR
- 量化工具链完善：NNCF（PTQ/QAT），小样本即可提速

#### 2. 先用Python体验一下

##### 2.1 准备工作

- 创建虚拟环境

  ```bash
  python -m venv .try_openvino
  source ./.try_openvino/bin/activate
  ```

  

- Python 依赖
  ```bash
  pip install -U openvino openvino-dev nncf
  
  python -c "import openvino as ov; print(ov.get_version())"
  ```

- 选择轻量分类模型：mobilenet-v2-pytorch, 使用 OMZ 工具一键下载与转换
  ```bash
  omz_downloader --name mobilenet-v2-pytorch
  
  omz_converter  --name mobilenet-v2-pytorch --precisions FP32
  ```
- 可选下载小数据集用于演示/量化
  - omz_data_downloader --name imagenette2-320 --output_dir datasets

##### 2.2 开始演示

Python 推理

```python
import openvino as ov
import numpy as np
import cv2, json, os

core = ov.Core()
model_path = "public/mobilenet-v2-pytorch/FP32/mobilenet-v2-pytorch.xml"
model = core.read_model(model_path)

compiled = core.compile_model(model, device_name="CPU", config={
    "PERFORMANCE_HINT": "LATENCY",  # 单请求低时延；批量吞吐改为 THROUGHPUT
    "NUM_STREAMS": "1",            
    "CACHE_DIR": "./ov_cache"       # 编译缓存，加速二次加载
})

input_node = compiled.input(0)

# 读取图片并预处理，同学们这个还是之前的ONNXRUNTIME演示的处理方法，是一致的
img = cv2.imread("cat.jpg")
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = cv2.resize(img, (224, 224))
x = img.astype(np.float32) / 255.0
mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)
x = ((x - mean) / std).transpose(2,0,1)[None]  # 同学们这一步是Python版本的转换到NCHW，大家可以对比一下

# 同步推理
output = compiled([x])[compiled.output(0)]
top5 = np.argsort(output, axis=1)[:, -5:][0][::-1]
print("Top-5 class ids:", top5.tolist())
```

讲者备注
- 初次运行会有编译耗时，第二次因 CACHE_DIR 明显更快。
- 想要更高吞吐：把 HINT 改为 THROUGHPUT，并发用 NUM_STREAMS="AUTO"。

—  
CPU 基准测试（benchmark_app）
常用命令

- benchmark_app -m public/mobilenet-v2-pytorch/FP32/mobilenet-v2-pytorch.xml -d CPU -t 10 -api async -hint throughput -streams AUTO
- 低时延对比：把 -hint latency -streams 1
输出关注
- Median latency、Throughput (FPS)、Total inference time

讲者备注
- 用两套参数（latency vs throughput）现场对比数据与差异。

—  
CPU 调优要点（入门即可）
- 性能目标
  - 低时延：PERFORMANCE_HINT=LATENCY，NUM_STREAMS=1
  - 高吞吐：PERFORMANCE_HINT=THROUGHPUT，NUM_STREAMS=AUTO（或 2/4）
- 并发与数据管道
  - 异步 API 能叠加请求；与数据加载并行更香
- 形状与缓存
  - 固定输入形状更易优化；启用 CACHE_DIR 加速二次加载
- 可选（进阶）
  - INFERENCE_NUM_THREADS（控制线程数，按物理核数微调）

讲者备注
- 简单有效的组合：THROUGHPUT + NUM_STREAMS=AUTO（演示吞吐提升）。

—  
INT8 量化（NNCF · PTQ）
- 目的：不重新训练，快速把 FP32 模型压到 INT8，提高 CPU 吞吐并省电
- 典型收益：1.5–3x 吞吐提升（依模型而定）
- 校准数据：几十到几百张“同分布”图片即可

最简 PTQ 示例（以 mobilenet-v2-pytorch 为例）
```python
import openvino as ov, nncf
import numpy as np, cv2, glob, os
from pathlib import Path

core = ov.Core()
ir_path = "public/mobilenet-v2-pytorch/FP32/mobilenet-v2-pytorch.xml"
model = core.read_model(ir_path)

# 准备少量校准图片（使用 imagenette2-320 验证集）
img_dir = Path("datasets/imagenette2-320/val")  # 若未下载，请先运行 omz_dataset_downloader
paths = list(img_dir.rglob("*.JPEG"))[:200]     # 取 200 张即可起步

def gen():
    for p in paths:
        img = cv2.cvtColor(cv2.imread(str(p)), cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (224, 224))
        x = img.astype(np.float32) / 255.0
        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)
        x = ((x - mean) / std).transpose(2,0,1)[None]
        yield x

calib_ds = nncf.Dataset(gen())
int8_model = nncf.quantize(model, calib_ds)
ov.serialize(int8_model, "mobilenet_v2_int8.xml")
print("Saved INT8 model: mobilenet_v2_int8.xml")
```

**对比脚本**

```bash
#!/usr/bin/env bash
set -e
MODEL_FP32="public/mobilenet-v2-pytorch/FP32/mobilenet-v2-pytorch.xml"
MODEL_INT8="mobilenet_v2_int8.xml"

mkdir -p reports

echo "== Warmup FP32 =="
benchmark_app -m "$MODEL_FP32" -d CPU -api sync -hint latency -streams 1 -niter 20 >/dev/null

echo "== FP32 Latency =="
benchmark_app -m "$MODEL_FP32" -d CPU -api sync -hint latency -streams 1 -niter 500 \
  -report_type average_counters -report_folder reports/fp32_lat

echo "== FP32 Throughput (AUTO) =="
benchmark_app -m "$MODEL_FP32" -d CPU -api async -hint throughput -streams AUTO -t 15 \
  -report_type average_counters -report_folder reports/fp32_thr_auto

for S in 2 4; do
  echo "== FP32 Throughput (streams=$S) =="
  benchmark_app -m "$MODEL_FP32" -d CPU -api async -hint throughput -streams $S -t 15 \
    -report_type average_counters -report_folder "reports/fp32_thr_s${S}"
done

if [ -f "$MODEL_INT8" ]; then
  echo "== Warmup INT8 =="
  benchmark_app -m "$MODEL_INT8" -d CPU -api sync -hint latency -streams 1 -niter 20 >/dev/null

  echo "== INT8 Latency =="
  benchmark_app -m "$MODEL_INT8" -d CPU -api sync -hint latency -streams 1 -niter 500 \
    -report_type average_counters -report_folder reports/int8_lat

  echo "== INT8 Throughput (AUTO) =="
  benchmark_app -m "$MODEL_INT8" -d CPU -api async -hint throughput -streams AUTO -t 15 \
    -report_type average_counters -report_folder reports/int8_thr_auto

  for S in 2 4; do
    echo "== INT8 Throughput (streams=$S) =="
    benchmark_app -m "$MODEL_INT8" -d CPU -api async -hint throughput -streams $S -t 15 \
      -report_type average_counters -report_folder "reports/int8_thr_s${S}"
  done
else
  echo "提示：未找到 INT8 模型（$MODEL_INT8），如需对比请先完成量化。"
fi
```



