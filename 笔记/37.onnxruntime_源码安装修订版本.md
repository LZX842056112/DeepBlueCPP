从源码编译安装 ONNX Runtime（CPU 版，含完整 C++ 和 Python 支持）的可执行手册。按步骤逐条照做即可完成构建、安装与验证。

目标产物

-  • C++: 可用的共享库和头文件（libonnxruntime.so、include/…）
-  • Python: 本机 Python 轮子（onnxruntime-*.whl）已安装，可直接 import 使用

注意

-  • 全程 CPU 版本（无需 CUDA/TensorRT）。
-  • 构建耗时取决于机器配置，一般 60 分钟。

### 1) 安装系统依赖

```
sudo apt update

sudo apt upgrade

sudo apt install -y \
  build-essential cmake git ninja-build \
  python3 python3-venv python3-dev python3-pip \
  curl wget unzip patchelf pkg-config
```

### 2) 获取源码

```
mkdir -p ~/onnxruntime_install/src && cd ~/onnxruntime_install/src
git clone --recursive https://github.com/microsoft/onnxruntime.git
cd onnxruntime
```

### 3) 准备 Python 虚拟环境

```
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip setuptools wheel packaging
pip install -U numpy
pip install -U onnx
```

### 4) 开始构建（同时生成 C++ 库与 Python wheel）

说明：

-  • 使用官方 build.sh 包装脚本（会自动拉取/更新子模块、配置并构建）

执行：

```
./build.sh \
  --config Release \
  --build_shared_lib \
  --build_wheel \
  --skip_tests \
  --cmake_extra_defines CMAKE_INSTALL_PREFIX=/usr/local
```

构建完成后，关键产物位置（默认）， 确认有以下目录和文件，如果没有表明构建是不完整的。

-  • C++ 构建目录：build/Linux/Release
-  • Python wheel：build/Linux/Release/dist/onnxruntime-*.whl

### 5) 安装产物

-   安装 C++ 头文件与库到 INSTALL_PREFIX（第 4 步已配置好前缀）

```
cmake --build build/Linux/Release --target install
```

-  让运行时能找到动态库

```
sudo ldconfig
```

-   安装 Python 轮子
```
pip install build/Linux/Release/dist/onnxruntime-*.whl

python -c "import onnxruntime as ort; print('ORT Python version:', ort.__version__, '| device:', ort.get_device())"
# 预期输出：device: CPU
```

### 6) 用 Python 进行验证推理

我们先用 onnx 生成一个非常小的模型：y = x + [1,2,3]，再用 onnxruntime 推理。

```
python - <<'PY'
import onnx, numpy as np
import onnxruntime as ort
from onnx import helper, TensorProto

# 1) 构造一个简单模型 y = x + b
X = helper.make_tensor_value_info("x", TensorProto.FLOAT, [3])
Y = helper.make_tensor_value_info("y", TensorProto.FLOAT, [3])
b = helper.make_tensor("b", TensorProto.FLOAT, [3], [1.0, 2.0, 3.0])
node = helper.make_node("Add", inputs=["x", "b"], outputs=["y"])
graph = helper.make_graph([node], "add_graph", [X], [Y], [b])
model = helper.make_model(graph, opset_imports=[helper.make_opsetid("", 13)])
onnx.save(model, "toy.onnx")

# 2) 使用 onnxruntime 进行推理
sess = ort.InferenceSession("toy.onnx", providers=["CPUExecutionProvider"])
inp = np.array([10, 20, 30], dtype=np.float32)
out = sess.run(None, {"x": inp})[0]
print("Input: ", inp)
print("Output:", out)  # 期望 [11, 22, 33]
PY
```

看到输出为 [11, 22, 33] 即 Python 验证成功。

### 7) 用 C++ 进行验证推理

复用上一步生成的 toy.onnx，写一个最小 C++ 示例并编译运行。

-  • 新建 main.cpp：

```
cat > main.cpp <<'CPP'
#include <iostream>
#include <vector>
#include <onnxruntime_cxx_api.h>

int main() {
  Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "demo");
  Ort::SessionOptions so;
  // 默认为 CPU EP
  Ort::Session session(env, "toy.onnx", so);

  Ort::AllocatorWithDefaultOptions allocator;
  auto input_name  = session.GetInputNameAllocated(0, allocator);
  auto output_name = session.GetOutputNameAllocated(0, allocator);

  std::vector<float> input{10.f, 20.f, 30.f};
  std::vector<int64_t> shape{3};
  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
  Ort::Value input_tensor = Ort::Value::CreateTensor<float>(memory_info, input.data(), input.size(), shape.data(), shape.size());

  const char* input_names[]  = { input_name.get()  };
  const char* output_names[] = { output_name.get() };

  auto output_tensors = session.Run(Ort::RunOptions{nullptr},
                                    input_names,  &input_tensor, 1,
                                    output_names, 1);
  float* y = output_tensors.front().GetTensorMutableData<float>();
  std::cout << "Output: [" << y[0] << ", " << y[1] << ", " << y[2] << "]\n";
  return 0;
}
CPP
```

-  • 编译并运行（使用你刚才安装的头文件与库）：0,1,2,3，Fast

```
g++ -O2 -std=c++17 main.cpp \
  -I/usr/local/include/onnxruntime \
  -L/usr/local/lib -lonnxruntime \
  -o demo_ort

./demo_ort
# 预期输出：Output: [11, 22, 33]
```

到这里，C++ 与 Python 的功能验证都通过了。



上面的全部验证完成，表明onnxruntime开发环境搭建完成。



### 8) 常见问题排查

构建不成功时

-  • 切换到稳定版本
   -  • 在第 2 步 clone 后 git checkout vX.Y.Z（例如 v1.18.0），可减少主干分支变动带来的不确定性。

至此，你和同学们已经能够在干净的 Ubuntu 上，从源码完成 ONNX Runtime（CPU）的完整 C++ 与 Python 构建、安装与验证