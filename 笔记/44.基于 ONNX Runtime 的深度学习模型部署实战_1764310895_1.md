# è¯¾ç¨‹é¡¹ç›®ï¼šåŸºäº ONNX Runtime çš„æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²å®æˆ˜

**é¡¹ç›®ç›®æ ‡ï¼š** è®­ç»ƒä¸€ä¸ªæ‰‹å†™æ•°å­—è¯†åˆ«æ¨¡å‹ (MNIST)ï¼Œå¹¶å°†å…¶éƒ¨ç½²åˆ° Python å’Œ C++ ç¯å¢ƒä¸­ã€‚

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒä¸å¯¼å‡º

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import torch.onnx
import os
import time

EPOCHS = 10
BATCH_SIZE = 64
TEST_BATCH_SIZE = 1000
LEARNING_RATE = 0.01
DEVICE = torch.device("cpu")

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(kernel_size=2)
        self.fc = nn.Linear(10 * 12 * 12, 10)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = x.view(-1, 10 * 12 * 12)
        x = self.fc(x)
        return x

def train(model, train_loader, optimizer, criterion, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(DEVICE), target.to(DEVICE)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                  f'({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}')

def test(model, test_loader, criterion):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(DEVICE), target.to(DEVICE)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)
    
    print(f'\nTest set: Average loss: {test_loss:.4f}, '
          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\n')
    return accuracy

def main():
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒä»»åŠ¡ï¼Œä½¿ç”¨è®¾å¤‡: {DEVICE}")
    

    # mean=0.1307, std=0.3081
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)

    model = SimpleCNN().to(DEVICE)
    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.5)
    criterion = nn.CrossEntropyLoss()

    best_accuracy = 0.0
    
    start_time = time.time()
    
    for epoch in range(1, EPOCHS + 1):
        train(model, train_loader, optimizer, criterion, epoch)
        current_accuracy = test(model, test_loader, criterion)
        
        if current_accuracy > best_accuracy:
            best_accuracy = current_accuracy
            torch.save(model.state_dict(), "mnist_cnn_best.pth")
            print(f"å‘ç°æ–°é«˜å‡†ç¡®ç‡ ({best_accuracy:.2f}%)ï¼Œæ¨¡å‹æƒé‡å·²ä¿å­˜ã€‚")
    
    print(f"è®­ç»ƒç»“æŸï¼Œè€—æ—¶: {time.time() - start_time:.1f}ç§’")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")

    print("-" * 30)
    print("æ­£åœ¨å¯¼å‡ºæœ€ä½³æ¨¡å‹åˆ° ONNX...")
    
    model.load_state_dict(torch.load("mnist_cnn_best.pth"))
    model.eval()
    model.to('cpu') # å¯¼å‡ºè¿‡ç¨‹å»ºè®®åœ¨ CPU ä¸Šè¿›è¡Œï¼Œé€šç”¨æ€§æ›´å¥½

    dummy_input = torch.randn(1, 1, 28, 28)
    output_onnx_name = "mnist_cnn.onnx"
    
    torch.onnx.export(model, 
                      dummy_input, 
                      output_onnx_name, 
                      input_names=['input'], 
                      output_names=['output'])
    
    print(f"ONNX æ¨¡å‹å·²ç”Ÿæˆ: {output_onnx_name}")

if __name__ == '__main__':
    main()
```

-----

## ç¬¬äºŒéƒ¨åˆ†ï¼šå‡†å¤‡æµ‹è¯•å›¾ç‰‡

```python
import cv2
import numpy as np

img = np.zeros((28, 28), dtype=np.uint8)

cv2.putText(img, "5", (4, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255), 2)

cv2.imwrite("test_digit.png", img)
print("æµ‹è¯•å›¾ç‰‡ test_digit.png (æ•°å­—5) å·²ç”Ÿæˆ")
```

-----

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šPython ç‰ˆæœ¬æ¨ç† (Python Inference)

```python
import onnxruntime as ort
import numpy as np
import cv2

def preprocess(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        raise Exception("æ— æ³•è¯»å–å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥è·¯å¾„")

    img = cv2.resize(img, (28, 28))

    img = img.astype(np.float32) / 255.0
    
    # mean=0.1307, std=0.3081
    img = (img - 0.1307) / 0.3081

    # 5. å¢åŠ ç»´åº¦ (H, W) -> (1, 1, H, W), è¦æƒ³æ˜ç™½
    img = np.expand_dims(img, axis=0)
    img = np.expand_dims(img, axis=0)
    
    return img

def main():
    onnx_model_path = "mnist_cnn.onnx"
    image_path = "test_digit.png"

    try:
        session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹ã€‚è¯·ç¡®ä¿å·²è¿è¡Œ 1_train_export.pyã€‚è¯¦æƒ…: {e}")
        return

    input_tensor = preprocess(image_path)
    
    input_name = session.get_inputs()[0].name  # 'input'
    output_name = session.get_outputs()[0].name # 'output'

    outputs = session.run([output_name], {input_name: input_tensor})
    
    logits = outputs[0] # åŸå§‹è¾“å‡º
    probabilities = np.exp(logits) / np.sum(np.exp(logits)) # Softmax (å¯é€‰ï¼Œçœ‹æ¦‚ç‡)
    prediction = np.argmax(logits)
    
    print(f"ç½®ä¿¡åº¦: {probabilities[0][prediction]:.4f}")
    print("predication: {prediction}")


if __name__ == "__main__":
    main()
```

-----

## âš™ï¸ ç¬¬å››éƒ¨åˆ†ï¼šC++ ç‰ˆæœ¬æ¨ç† (C++ Inference)

### 1\. ç¯å¢ƒå‡†å¤‡

```cmake
cmake_minimum_required(VERSION 3.10)
project(MnistCpp)

set(CMAKE_CXX_STANDARD 17)

find_package(OpenCV REQUIRED)

find_path(ONNXRUNTIME_INCLUDE_DIR 
    NAMES onnxruntime_cxx_api.h
    PATHS /usr/local/include /usr/include
    PATH_SUFFIXES onnxruntime core/session
)

find_library(ONNXRUNTIME_LIB 
    NAMES onnxruntime
    PATHS /usr/local/lib /usr/lib
)

if(NOT ONNXRUNTIME_INCLUDE_DIR OR NOT ONNXRUNTIME_LIB)
    message(FATAL_ERROR "æœªæ‰¾åˆ° ONNX Runtimeï¼è¯·ç¡®è®¤å·²å®‰è£…åˆ° /usr/local æˆ–è®¾ç½®ç›¸å…³è·¯å¾„ã€‚")
else()
    message(STATUS "Found ONNX Runtime Include: ${ONNXRUNTIME_INCLUDE_DIR}")
    message(STATUS "Found ONNX Runtime Lib: ${ONNXRUNTIME_LIB}")
endif()

add_executable(mnist_demo main.cpp)

target_include_directories(mnist_demo PRIVATE ${ONNXRUNTIME_INCLUDE_DIR})
target_link_libraries(mnist_demo ${OpenCV_LIBS} ${ONNXRUNTIME_LIB})
```

### 3\. C++ æ ¸å¿ƒä»£ç 

```cpp
#include <iostream>
#include <vector>
#include <numeric>
#include <opencv2/opencv.hpp>
#include <onnxruntime_cxx_api.h>

std::vector<float> preprocess(const cv::Mat& img) {
    cv::Mat processed;
    
    cv::resize(img, processed, cv::Size(28, 28));
    
    processed.convertTo(processed, CV_32F, 1.0 / 255.0);

    // 3. æ ‡å‡†åŒ– (Mean=0.1307, Std=0.3081)
    processed = (processed - 0.1307) / 0.3081;

    // 4. Flatten (å±•å¼€ä¸ºä¸€ç»´å‘é‡)
    // OpenCV Mat æ˜¯è¡Œä¼˜å…ˆå­˜å‚¨ï¼Œå¯ä»¥ç›´æ¥æ‹·è´
    std::vector<float> input_tensor_values;
    if (processed.isContinuous()) {
        input_tensor_values.assign((float*)processed.datastart, (float*)processed.dataend);
    } else {
        for (int i = 0; i < processed.rows; ++i)
            input_tensor_values.insert(input_tensor_values.end(), processed.ptr<float>(i), processed.ptr<float>(i) + processed.cols);
    }
    return input_tensor_values;
}

int main() {
    std::string model_path = "../../mnist_cnn.onnx"; 
    std::string image_path = "../../test_digit.png";

    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "MnistCpp");
    Ort::SessionOptions session_options;
    Ort::Session session(env, model_path.c_str(), session_options);

    cv::Mat img = cv::imread(image_path, cv::IMREAD_GRAYSCALE);
    if (img.empty()) {
        std::cerr << "Error: Could not read image from " << image_path << std::endl;
        return -1;
    }

    std::vector<float> input_tensor_values = preprocess(img);
    
    std::vector<int64_t> input_shape = {1, 1, 28, 28};
    size_t input_tensor_size = 28 * 28;

    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        memory_info, 
        input_tensor_values.data(), 
        input_tensor_size, 
        input_shape.data(), 
        input_shape.size()
    );

    const char* input_names[] = {"input"};
    const char* output_names[] = {"output"};

    std::cout << "Running Inference..." << std::endl;
    auto output_tensors = session.Run(
        Ort::RunOptions{nullptr}, 
        input_names, 
        &input_tensor, 
        1, 
        output_names, 
        1
    );

    float* floatarr = output_tensors[0].GetTensorMutableData<float>();
    
    auto max_it = std::max_element(floatarr, floatarr + 10);
    int prediction = std::distance(floatarr, max_it);
    float confidence = *max_it;

    std::cout << "-----------------------" << std::endl;
    std::cout << "Prediction Result: " << prediction << std::endl;
    std::cout << "Raw Logit Value:   " << confidence << std::endl;
    std::cout << "-----------------------" << std::endl;

    return 0;
}
```

### 4\. ç¼–è¯‘ä¸è¿è¡Œ

```bash
cd cpp_deploy
mkdir build
cd build
cmake ..
make
./mnist_demo
```

## ç¬¬äº”éƒ¨åˆ†ï¼šOpenVINO 2025.3.0 éƒ¨ç½²å®æˆ˜

OpenVINO åœ¨ Intel ç¡¬ä»¶ï¼ˆCPUã€iGPUã€Arc dGPUï¼‰ä¸Šé€šå¸¸èƒ½æä¾›æ¯”é€šç”¨ ONNX Runtime æ›´é«˜çš„æ¨ç†æ€§èƒ½ã€‚

### 1\. Python ç‰ˆæœ¬ (OpenVINO)

é¦–å…ˆå®‰è£… OpenVINO å¼€å‘åŒ…ï¼š

```bash
pip install openvino==2025.3.0
```

ä»£ç å®ç°ï¼š

```python
import openvino as ov
import numpy as np
import cv2
import time

def preprocess(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        raise Exception("æ— æ³•è¯»å–å›¾ç‰‡")

    img = cv2.resize(img, (28, 28))
    img = img.astype(np.float32) / 255.0
    # å½’ä¸€åŒ– (Mean=0.1307, Std=0.3081)
    img = (img - 0.1307) / 0.3081
    
    # (N, C, H, W) -> (1, 1, 28, 28)
    img = np.expand_dims(img, axis=0)
    img = np.expand_dims(img, axis=0)
    return img

def main():
    model_path = "mnist_cnn.onnx"
    image_path = "test_digit.png"

    core = ov.Core()

    compiled_model = core.compile_model(model_path, device_name="CPU")

    infer_request = compiled_model.create_infer_request()

    # 3. å‡†å¤‡æ•°æ®
    input_tensor_data = preprocess(image_path)
    
    input_node = compiled_model.inputs[0]
    output_node = compiled_model.outputs[0]

    # 4. å¼€å§‹æ¨ç†
    start_time = time.time()
    results = infer_request.infer({input_node: input_tensor_data})
    end_time = time.time()

    # 5. åå¤„ç†ç»“æœ
    logits = results[output_node][0]
    prediction = np.argmax(logits)
    confidence = np.exp(logits[prediction]) / np.sum(np.exp(logits))

    print(f"æ¨ç†è€—æ—¶: {(end_time - start_time)*1000:.2f} ms")
    print(f"é¢„æµ‹ç»“æœ: {prediction}")
    print(f"ç½®ä¿¡åº¦:   {confidence:.4f}")

if __name__ == "__main__":
    main()
```

-----

### 2\. C++ ç‰ˆæœ¬ (OpenVINO)

#### 2.1 CMakeLists.txt é…ç½®

éœ€è¦åœ¨ä¹‹å‰çš„ CMake åŸºç¡€ä¸Šå¢åŠ  OpenVINO çš„æŸ¥æ‰¾ï¼š

```cmake
cmake_minimum_required(VERSION 3.10)
project(MnistOpenVINO)

set(CMAKE_CXX_STANDARD 17)

find_package(OpenCV REQUIRED)

find_package(OpenVINO REQUIRED)

add_executable(mnist_ov main_ov.cpp)

target_link_libraries(mnist_ov ${OpenCV_LIBS} openvino::runtime)
```

#### 2.2 C++ æ ¸å¿ƒä»£ç  (`main_ov.cpp`)

```cpp
#include <iostream>
#include <vector>
#include <algorithm>
#include <opencv2/opencv.hpp>
#include <openvino/openvino.hpp> // åŒå­¦ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªå¤´æ–‡ä»¶æ˜¯å¦éœ€è¦è°ƒæ•´

std::vector<float> preprocess(const cv::Mat& img) {
    cv::Mat processed;
    cv::resize(img, processed, cv::Size(28, 28));
    processed.convertTo(processed, CV_32F, 1.0 / 255.0);
    processed = (processed - 0.1307) / 0.3081;

    std::vector<float> input_data;
    if (processed.isContinuous()) {
        input_data.assign((float*)processed.datastart, (float*)processed.dataend);
    } else {
        for (int i = 0; i < processed.rows; ++i)
            input_data.insert(input_data.end(), processed.ptr<float>(i), processed.ptr<float>(i) + processed.cols);
    }
    return input_data;
}

int main() {
    std::string model_path = "../../mnist_cnn.onnx";
    std::string image_path = "../../test_digit.png";

    ov::Core core;
    
    std::vector<std::string> available_devices = core.get_available_devices();
    std::cout << "Available OpenVINO devices: ";
    for (const auto& dev : available_devices) std::cout << dev << " ";
    std::cout << std::endl;

    // 2. è¯»å–å¹¶ç¼–è¯‘æ¨¡å‹
    std::cout << "Loading model..." << std::endl;
    ov::CompiledModel compiled_model = core.compile_model(model_path, "CPU");
    ov::InferRequest infer_request = compiled_model.create_infer_request();

    // 3. å‡†å¤‡è¾“å…¥æ•°æ®
    cv::Mat img = cv::imread(image_path, cv::IMREAD_GRAYSCALE);
    if (img.empty()) {
        std::cerr << "Error loading image" << std::endl;
        return -1;
    }
    std::vector<float> input_data = preprocess(img);

    ov::Tensor input_tensor = infer_request.get_input_tensor(0);
    
    std::memcpy(input_tensor.data<float>(), input_data.data(), input_data.size() * sizeof(float));

    // 4. å¼€å§‹æ¨ç†
    std::cout << "Running Inference..." << std::endl;
    infer_request.infer();

    // 5. è·å–ç»“æœ
    const float* output_buffer = infer_request.get_output_tensor(0).data<float>();
    
    // æ‰¾åˆ°æœ€å¤§å€¼ç´¢å¼•
    auto max_it = std::max_element(output_buffer, output_buffer + 10);
    int prediction = std::distance(output_buffer, max_it);
    float score = *max_it;

    std::cout << "-----------------------" << std::endl;
    std::cout << "OpenVINO Prediction: " << prediction << std::endl;
    std::cout << "Raw Score:           " << score << std::endl;
    std::cout << "-----------------------" << std::endl;

    return 0;
}
```

### 3\. æŠ€æœ¯å¯¹æ¯”åˆ†æ (Analysis)

åœ¨éƒ¨ç½²ç¯èŠ‚ä¸­ï¼ŒONNX Runtime å’Œ OpenVINO å„æœ‰åƒç§‹ï¼Œä»¥ä¸‹æ˜¯é’ˆå¯¹æœ¬é¡¹ç›®çš„ç®€è¦åˆ†æï¼š

| ç‰¹æ€§ | ONNX Runtime (ORT) | OpenVINO |
| :--- | :--- | :--- |
| **é€šç”¨æ€§** | **é«˜**ã€‚è·¨å¹³å°ï¼ˆWindows/Linux/Mac/Android/iOSï¼‰ï¼Œæ”¯æŒç¡¬ä»¶å¹¿æ³›ï¼ˆCUDA/TensorRT/CoreMLç­‰ï¼‰ã€‚ | **ç‰¹å®š**ã€‚ä¸“æ³¨äº Intel ç¡¬ä»¶ç”Ÿæ€ï¼ˆXeon CPU, Core CPU, Arc GPU, NPUï¼‰ã€‚
| **æ€§èƒ½ (Intel CPU)** | ä¼˜ç§€ï¼Œä½†é»˜è®¤è°ƒåº¦ç•¥é€Šäº OpenVINOã€‚ | **æè‡´**ã€‚åˆ©ç”¨ AVX-512ã€AMX ç­‰æŒ‡ä»¤é›†ä¼˜åŒ–ï¼Œä¸”æ”¯æŒå¼‚æ„è°ƒåº¦ï¼ˆAuto Pluginï¼‰ã€‚ |

**å»ºè®®ï¼š**

* å¦‚æœä½ çš„åº”ç”¨éœ€è¦éƒ¨ç½²åœ¨ **å¤šç§ç¡¬ä»¶æ··åˆ** çš„ç¯å¢ƒä¸­ï¼ˆå¦‚åŒæ—¶æœ‰ NVIDIA æ˜¾å¡å’Œ AMD CPUï¼‰ï¼Œé¦–é€‰ **ONNX Runtime**ã€‚
* å¦‚æœä½ çš„å·¥æ§æœºæˆ–æœåŠ¡å™¨æ˜¯ **çº¯ Intel å¹³å°**ï¼ˆç‰¹åˆ«æ˜¯è¾¹ç¼˜è®¡ç®—ç›’å­ï¼Œå¦‚åŸºäº Intel N100 æˆ– Core i5/i7 çš„è®¾å¤‡ï¼‰ï¼Œä½¿ç”¨ **OpenVINO** ä¼šè·å¾—æ›´ä½çš„å»¶è¿Ÿå’Œæ›´é«˜çš„ååé‡ã€‚P99 æ—¶é—´ä¼šæ›´ä¼˜ã€‚



### ç¬¬å…­éƒ¨åˆ†ï¼šCUDA æ¨ç†

```c++
#include <iostream>
#include <vector>
#include <numeric>
#include <algorithm>
#include <opencv2/opencv.hpp>
#include <onnxruntime_cxx_api.h>

std::vector<float> preprocess(const cv::Mat& img) {
    cv::Mat processed;
    
    cv::resize(img, processed, cv::Size(28, 28));
    
    processed.convertTo(processed, CV_32F, 1.0 / 255.0);

    // è¿™é‡Œçš„æ•°å­¦è¿ç®—åœ¨ CPU ä¸Šå¯¹ 28x28 çŸ©é˜µæå¿«ï¼Œæ— éœ€ CUDA æ ¸å‡½æ•°
    processed = (processed - 0.1307) / 0.3081;

    // 4. Flatten (å±•å¼€ä¸ºä¸€ç»´å‘é‡)
    std::vector<float> input_tensor_values;
    if (processed.isContinuous()) {
        input_tensor_values.assign((float*)processed.datastart, (float*)processed.dataend);
    } else {
        for (int i = 0; i < processed.rows; ++i)
            input_tensor_values.insert(input_tensor_values.end(), processed.ptr<float>(i), processed.ptr<float>(i) + processed.cols);
    }
    return input_tensor_values;
}

int main() {
    std::string model_path = "../../mnist_cnn.onnx"; 
    std::string image_path = "../../test_digit.png";

    // --- å…³é”®ä¿®æ”¹ 1: åˆå§‹åŒ– ONNX ç¯å¢ƒ ---
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "MnistCuda");
    Ort::SessionOptions session_options;

    // è¿™æ®µä»£ç å‘Šè¯‰ ONNX Runtime ä¼˜å…ˆä½¿ç”¨ GPU è¿›è¡Œè®¡ç®—
    try {
        OrtCUDAProviderOptions cuda_options;
        cuda_options.device_id = 0; // ä½¿ç”¨ 0 å· GPU
        // æ˜¾å­˜æº¢å‡ºç­–ç•¥ç­‰å¯ä»¥æ ¹æ®éœ€è¦é…ç½®
        // cuda_options.arena_extend_strategy = 0; 
        
        session_options.AppendExecutionProvider_CUDA(cuda_options);
        std::cout << "CUDA Execution Provider enabled successfully." << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "Failed to enable CUDA provider: " << e.what() << std::endl;
        std::cerr << "Falling back to CPU." << std::endl;
    }

    // è¿™ä¸ªæ—¶å€™æ­¤æ—¶æ¨¡å‹ä¼šè¢«åŠ è½½åˆ° GPU æ˜¾å­˜
    Ort::Session session(env, model_path.c_str(), session_options);

    // è¯»å–å›¾ç‰‡ (CPU)
    cv::Mat img = cv::imread(image_path, cv::IMREAD_GRAYSCALE);
    if (img.empty()) {
        std::cerr << "Error: Could not read image from " << image_path << std::endl;
        return -1;
    }

    // é¢„å¤„ç† (CPU)
    // æ­¤æ—¶æ•°æ®åœ¨ CPU å†…å­˜ä¸­
    std::vector<float> input_tensor_values = preprocess(img);
    
    std::vector<int64_t> input_shape = {1, 1, 28, 28};
    size_t input_tensor_size = 28 * 28;
  
    // æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¾ç„¶ä½¿ç”¨ Cpu å†…å­˜åˆ›å»º Tensorï¼ŒONNX Runtime åœ¨è¿è¡Œ Run() æ—¶
    // ä¼šè‡ªåŠ¨å¤„ç† CPU -> GPU çš„æ•°æ®æ‹·è´ (Host to Device Copy)ã€‚
    // å¦‚æœè¿½æ±‚æè‡´æ€§èƒ½ï¼Œéœ€ä½¿ç”¨ Ort::MemoryInfo::CreateCuda å¹¶ä½¿ç”¨ cudaMalloc é¢„åˆ†é…æ˜¾å­˜ï¼Œ
    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        memory_info, 
        input_tensor_values.data(), 
        input_tensor_size, 
        input_shape.data(), 
        input_shape.size()
    );

    const char* input_names[] = {"input"};
    const char* output_names[] = {"output"};

    std::cout << "Running Inference on GPU..." << std::endl;
    
    // æ‰§è¡Œæ¨ç† (Compute on GPU)
    auto output_tensors = session.Run(
        Ort::RunOptions{nullptr}, 
        input_names, 
        &input_tensor, 
        1, 
        output_names, 
        1
    );

    // è·å–ç»“æœï¼Œè¿”å›åˆ° CPU
    float* floatarr = output_tensors[0].GetTensorMutableData<float>();
    
    auto max_it = std::max_element(floatarr, floatarr + 10);
    int prediction = std::distance(floatarr, max_it);
    float confidence = *max_it;

    std::cout << "-----------------------" << std::endl;
    std::cout << "Prediction Result: " << prediction << std::endl;
    std::cout << "Raw Logit Value:   " << confidence << std::endl;
    std::cout << "-----------------------" << std::endl;

    return 0;
}
```



CMakeLists.txt

```cmake
cmake_minimum_required(VERSION 3.17)
project(MnistCudaInference)

set(CMAKE_CXX_STANDARD 17)

find_package(OpenCV REQUIRED)

enable_language(CUDA)
find_package(CUDAToolkit REQUIRED)

set(ONNXRUNTIME_ROOT "/path/to/your/onnxruntime-linux-x64-gpu-1.16.0")

include_directories(${ONNXRUNTIME_ROOT}/include)
link_directories(${ONNXRUNTIME_ROOT}/lib)

add_executable(mnist_cuda main.cpp)

# --- 5. é“¾æ¥åº“ ---
target_link_libraries(mnist_cuda
    PRIVATE
    ${OpenCV_LIBS}
    onnxruntime           # é“¾æ¥ onnxruntime æ ¸å¿ƒåº“
    onnxruntime_providers_cuda  # é“¾æ¥ CUDA Provider
    CUDA::cudart          # é“¾æ¥ CUDA Runtime
)

# --- 6. è¿è¡Œæ—¶è·¯å¾„è®¾ç½® (Rpath) ---
# è¿™èƒ½ç¡®ä¿è¿è¡Œç¨‹åºæ—¶èƒ½æ‰¾åˆ° onnxruntime çš„ .so æ–‡ä»¶ï¼Œè€Œä¸éœ€è¦æ¯æ¬¡éƒ½ export LD_LIBRARY_PATH
set_target_properties(mnist_cuda PROPERTIES
    BUILD_RPATH "${ONNXRUNTIME_ROOT}/lib"
    INSTALL_RPATH "${ONNXRUNTIME_ROOT}/lib"
)
```

