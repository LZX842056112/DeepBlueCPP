1. 环境与 TensorRT 安装（Windows 11）
2. Python 验证代码
3. C++ 验证代码（Visual Studio

---

## 一、环境与 TensorRT 安装（Windows 11）

### 0. 预备条件检查

1. **确认有 NVIDIA GPU**
   - 右键开始菜单 → 设备管理器 → 展开“显示适配器”，看到 NVIDIA xxx 就可以。
2. **确认操作系统**
   - 设置 → 系统 → 关于 → 设备规格/系统类型：应为“64 位操作系统，基于 x64 的处理器”。

---

### 1. 安装 NVIDIA 显卡驱动

1. 打开 NVIDIA 官网驱动下载页面（比如搜索“nvidia driver download”）。
2. 选择你的显卡型号和 Windows 11 64-bit。
3. 下载并安装最新的 Studio Driver。
4. 安装结束后建议重启电脑。

> 驱动要与 CUDA 版本兼容，一般安装最新版驱动就能兼容当前 CUDA 11.8。

---

### 2. 安装 Visual Studio

1. 打开 Visual Studio 官网，下载 **Visual Studio 2019 或 2022 Community**
2. 运行安装程序，在“工作负载”界面中勾选：
   - “使用 C++ 的桌面开发”
3. 安装完成后重启或至少关闭并重新打开终端。

---

### 3. 安装 Python

1. 访问 Python 官网下载页面，选择一个与 TensorRT Python wheel 匹配的版本。
   - 对于 TensorRT 8.x，通常有 3.8 / 3.9 / 3.10 的 wheel。
   - 建议用 **Python 3.10**。
2. 勾选 “Add Python 3.10 to PATH”，点击 Install。
3. 安装后打开命令行（Win + R → 输入 `cmd`）：

```bash
python --version
pip --version
```

确认可以正常使用。

---

### 4. 安装 CUDA Toolkit

1. 打开 NVIDIA CUDA Toolkit 下载页。
2. 选择：
   - Operating System: Windows
   - Architecture: x86_64
   - Version: Windows 11
   - CUDA Version: 11.8
3. 下载 **exe (local)** 安装程序并运行
4. 安装完后，打开新 cmd，检查：

```bash
nvcc --version
```

若能输出 CUDA 版本（例如 11.8），说明 CUDA 安装成功且环境变量已配置。

> CUDA 安装后会自动设置 `CUDA_PATH` 环境变量，例如：  
> `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8`

---

### 5. 安装 cuDNN

1. 前往 NVIDIA cuDNN 下载页, 同学们要注册NVIDIA 开发者账号：

   - 下载与 **CUDA 11.x 对应的 cuDNN 8.x for Windows** 压缩包。
2. 解压下载得到的压缩包，比如到 `D:\downloads\cudnn-windows-x86_64-8.x.x.x_cuda11-archive`。
3. 里面一般有三个目录：`bin`、`include`、`lib`
4. 将它们拷贝到 CUDA 安装目录对应位置（需要管理员权限）：

   假定 CUDA 安装在：
   `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\`

   那么：
   - 把 cuDNN 的 `bin` 里的文件复制到：
     `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin`
   - 把 cuDNN 的 `include` 里的文件复制到：
     `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\include`
   - 把 cuDNN 的 `lib` 里的文件复制到：
     `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\lib\x64`

5. 检查 `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin` 目录下是否有 `cudnn64_*.dll` 文件。

---

### 6. 安装 TensorRT

1. 打开 TensorRT 下载页面（搜索 “TensorRT download”）。
2. 在下载列表中选择：
   - 操作系统：Windows
   - 平台：x86_64
   - CUDA 版本：选择与你安装的 CUDA 一致（示例：CUDA 11.x）
3. 下载类似命名的 zip 文件，例如：

   - `TensorRT-8.6.x.x.Windows10.x86_64.cuda-11.x.cudnn8.zip`
4. 解压到一个固定路径，例如：

   ```text
   C:\TensorRT-8.6.1.6\
   ```

   解压后你会看到目录结构大致为：
   - `bin/`
   - `include/`
   - `lib/`
   - `samples/`
   - `python/`

---

### 7. 配置环境变量，这个地方一定要配置对

1. 右键开始菜单 → “系统” → 右侧点击“高级系统设置” → “环境变量”。
2. 在“系统变量”中添加或修改：

   - 新建 `TENSORRT_DIR`：
     - 值：`C:\TensorRT-8.6.1.6`  （根据你实际解压路径来调整）
   - 编辑 `Path`（系统变量）：
     - 添加：`%TENSORRT_DIR%\lib`
     - CUDA 已经添加了类似：`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin`，如果没有，可手动加上。

3. 确认并关闭对话框。
4. 重新打开一个新的 cmd，测试 `nvinfer` 动态库是否可见：

```bash
where nvinfer.dll（如果你用的是Powershell，改成where.exe nvinfer.dll)
```

如果能看到类似 `C:\TensorRT-8.6.1.6\lib\nvinfer.dll`，说明环境路径配置正常。

---

### 8. 安装 TensorRT Python 包

1. 在刚才解压的 `C:\TensorRT-8.6.1.6\` 目录中，找到 `python` 子目录，下一级通常有不同 Python 版本号的目录，例如：

   - `python\` 或 `python\py3.10` 等。
2. 进入包含 `.whl` 文件的目录，里面会有类似：

   ```text
   tensorrt-8.6.1-cp310-none-win_amd64.whl
   tensorrt-10.12.0.36-cp38-none-win_amd64.whl
   # 名称和版本以实际为准
   ```

3. 在该目录中打开命令行（资源管理器地址栏输入 `cmd` 回车），执行：

```bash
pip install tensorrt-8.6.1-cp310-none-win_amd64.whl
pip install tensorrt-10.12.0.36-cp38-none-win_amd64.whl
# 上面文件名请以你看到的为准
```

4. 安装完成后，在任意目录打开 Python：

```bash
python -c "import tensorrt as trt; print(trt.__version__)"
```

能打印出版本号（例如 8.6.1），说明 Python 包已安装成功。

> 到这里：驱动 + CUDA + cuDNN + TensorRT（C++ & Python）环境都已就绪，接下来用代码验证。

---

## 二、Python 验证程序

下面这个脚本会做几件事：

1. 导入 TensorRT 并打印版本；
2. 创建一个简单的网络（1 个输入 + 1 个 identity 层）；
3. 用 Builder 构建一个序列化引擎；
4. 反序列化得到 Engine，并打印绑定数量。

这样可以验证：

- TensorRT Python binding 正常；
- CUDA / cuDNN 底层调用正常；
- Builder/Runtime API 可用。

### Python 验证脚本：`verify_tensorrt_python.py`

```python
import tensorrt as trt

def main():
    # 1. 打印 TensorRT 版本
    print("TensorRT Python version:", trt.__version__)

    # 2. 创建 Logger
    logger = trt.Logger(trt.Logger.INFO)

    # 3. 创建 Builder
    builder = trt.Builder(logger)
    # 使用显式 batch 模式（TensorRT 8+ 推荐）
    EXPLICIT_BATCH = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    network = builder.create_network(EXPLICIT_BATCH)

    # 4. 定义一个简单网络：输入 -> Identity -> 输出
    input_tensor = network.add_input(
        name="input",
        dtype=trt.float32,
        shape=(1, 1, 1)  # NCHW 中的 CHW（N 由显式 batch 控制）
    )

    identity_layer = network.add_identity(input_tensor)
    identity_layer.name = "identity_layer"

    network.mark_output(identity_layer.get_output(0))

    # 5. 创建 BuilderConfig 并构建引擎
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 20  # 1MB，示例用，足够这个小网络

    print("Building TensorRT engine...")
    serialized_engine = builder.build_serialized_network(network, config)
    if serialized_engine is None:
        print("Failed to build serialized engine. Please check CUDA/cuDNN/TensorRT installation.")
        return

    # 6. 使用 Runtime 反序列化引擎
    runtime = trt.Runtime(logger)
    engine = runtime.deserialize_cuda_engine(serialized_engine)

    if engine is None:
        print("Failed to deserialize engine.")
    else:
        print("Engine built and deserialized successfully.")
        print("Number of bindings:", engine.num_bindings)
        for i in range(engine.num_bindings):
            name = engine.get_binding_name(i)
            is_input = engine.binding_is_input(i)
            print(f"  Binding {i}: name={name}, is_input={is_input}, shape={engine.get_binding_shape(i)}")

if __name__ == "__main__":
    main()
```

运行方法（在 `.py` 文件所在目录打开 cmd）：

```bash
python verify_tensorrt_python.py
```

如果最后看到类似：

```text
TensorRT Python version: 8.6.1
Building TensorRT engine...
Engine built and deserialized successfully.
Number of bindings: 2
  Binding 0: name=input, is_input=True, shape=(1, 1, 1)
  Binding 1: name=identity_layer_output_0, is_input=False, shape=(1, 1, 1)
```

说明 Python 侧 TensorRT 工作正常。

---

## 三、C++ 验证程序（Visual Studio）

这部分演示：

1. 如何在 Visual Studio 里配置包含目录 / 库目录；
2. 用 C++ API 构建一个同样的“输入 -> Identity -> 输出”的 TensorRT 引擎；
3. 成功构建即说明 C++ 开发环境配置正确。

### 1. 创建 Visual Studio 工程

1. 打开 Visual Studio → “创建新项目”。
2. 选择：
   - “控制台应用”（C++）/“Console App”，平台：Windows。
3. 填写工程名，比如 `TensorRTVerifyCpp`，选择存放位置，点击创建。

---

### 2. 配置 C/C++ / 链接器路径

假定：

- TensorRT 解压在：`C:\TensorRT-8.6.1.6`
- CUDA 安装在：`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8`

右键你的工程 → “属性（Properties）”，确保 **“配置”选择为“所有配置（All Configurations）”，“平台”为 x64**，然后：

1. **C/C++ → 常规 → 附加包含目录（Additional Include Directories）**
   - 添加：
     - `C:\TensorRT-8.6.1.6\include`
     - `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\include`

2. **链接器 → 常规 → 附加库目录（Additional Library Directories）**
   - 添加：
     - `C:\TensorRT-8.6.1.6\lib`
     - `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\lib\x64`

3. **链接器 → 输入 → 附加依赖项（Additional Dependencies）**
   - 添加（如果列表中没有）：
     - `nvinfer.lib`
     - `cudart.lib`
   - （可选，如后续用到插件：`nvinfer_plugin.lib` 等）

点击“应用”/“确定”。

---

### 3. C++ 验证代码：`main.cpp`

把工程自带的 `main.cpp` 内容替换为以下代码：

```cpp
#include <iostream>
#include <NvInfer.h>

using namespace nvinfer1;

// 简单的 Logger：只打印 Info 及以上级别日志
class SimpleLogger : public ILogger {
public:
    void log(Severity severity, const char* msg) noexcept override {
        // 只打印 Info 及以上（忽略 Verbose）
        if (severity <= Severity::kINFO) {
            std::cout << "[TensorRT] " << msg << std::endl;
        }
    }
};

int main() {
    SimpleLogger logger;

    std::cout << "Creating TensorRT builder..." << std::endl;
    IBuilder* builder = createInferBuilder(logger);
    if (!builder) {
        std::cerr << "Failed to create TensorRT builder. Check installation." << std::endl;
        return -1;
    }

    // 显式 batch 模式
    uint32_t explicitBatch = 1U << static_cast<uint32_t>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
    INetworkDefinition* network = builder->createNetworkV2(explicitBatch);
    if (!network) {
        std::cerr << "Failed to create network definition." << std::endl;
        builder->destroy();
        return -1;
    }

    // 创建一个输入张量，形状 (1,1,1)
    ITensor* input = network->addInput("input", DataType::kFLOAT, Dims3{1, 1, 1});
    if (!input) {
        std::cerr << "Failed to add input tensor." << std::endl;
        network->destroy();
        builder->destroy();
        return -1;
    }

    // Identity 层：输出 = 输入
    ILayer* identityLayer = network->addIdentity(*input);
    if (!identityLayer) {
        std::cerr << "Failed to add identity layer." << std::endl;
        network->destroy();
        builder->destroy();
        return -1;
    }
    identityLayer->setName("identity_layer");

    ITensor* output = identityLayer->getOutput(0);
    network->markOutput(*output);

    // 创建 BuilderConfig
    IBuilderConfig* config = builder->createBuilderConfig();
    if (!config) {
        std::cerr << "Failed to create builder config." << std::endl;
        network->destroy();
        builder->destroy();
        return -1;
    }
    config->setMaxWorkspaceSize(1 << 20);
    std::cout << "Building TensorRT engine..." << std::endl;
    IHostMemory* serializedEngine = builder->buildSerializedNetwork(*network, *config);
    if (!serializedEngine) {
        std::cerr << "Failed to build serialized engine. Check CUDA/cuDNN/TensorRT." << std::endl;
        config->destroy();
        network->destroy();
        builder->destroy();
        return -1;
    }

    std::cout << "Engine serialized successfully. Size = "
              << serializedEngine->size() << " bytes" << std::endl;

    // 反序列化引擎
    IRuntime* runtime = createInferRuntime(logger);
    if (!runtime) {
        std::cerr << "Failed to create runtime." << std::endl;
        serializedEngine->destroy();
        config->destroy();
        network->destroy();
        builder->destroy();
        return -1;
    }

    ICudaEngine* engine = runtime->deserializeCudaEngine(
        serializedEngine->data(), serializedEngine->size(), nullptr);
    if (!engine) {
        std::cerr << "Failed to deserialize engine." << std::endl;
        runtime->destroy();
        serializedEngine->destroy();
        config->destroy();
        network->destroy();
        builder->destroy();
        return -1;
    }

    std::cout << "Engine deserialized successfully." << std::endl;
    std::cout << "Number of bindings: " << engine->getNbBindings() << std::endl;
    for (int i = 0; i < engine->getNbBindings(); ++i) {
        auto dims = engine->getBindingDimensions(i);
        auto dtype = engine->getBindingDataType(i);
        bool isInput = engine->bindingIsInput(i);
        std::cout << "  Binding " << i
                  << " name=" << engine->getBindingName(i)
                  << ", isInput=" << isInput
                  << ", dims=(";
        for (int d = 0; d < dims.nbDims; ++d) {
            std::cout << dims.d[d];
            if (d + 1 < dims.nbDims) std::cout << ",";
        }
        std::cout << "), dtype=" << static_cast<int>(dtype) << std::endl;
    }

    // 释放资源
    engine->destroy();
    runtime->destroy();
    serializedEngine->destroy();
    config->destroy();
    network->destroy();
    builder->destroy();

    std::cout << "TensorRT C++ verification finished successfully." << std::endl;
    return 0;
}
```

### 4. 运行 C++ 验证

1. 确认工程平台是 x64（工具栏里有个 “x86 / x64” 选择）。
2. 菜单栏 → 生成 → 生成解决方案。
3. 如果编译通过，运行（Ctrl + F5 或点击“本地 Windows 调试器”）。
4. 若看到类似输出：

```text
Creating TensorRT builder...
[TensorRT] ... 若干 TensorRT 日志 ...
Building TensorRT engine...
Engine serialized successfully. Size = xxxx bytes
Engine deserialized successfully.
Number of bindings: 2
  Binding 0 name=input, isInput=1, dims=(1,1,1), dtype=0
  Binding 1 name=identity_layer_output_0, isInput=0, dims=(1,1,1), dtype=0
TensorRT C++ verification finished successfully.
```

说明：

- 头文件 / 库路径配置正确；
- 动态库（`nvinfer.dll`、`cudart64_xx.dll` 等）能被正确加载；
- TensorRT C++ API 可以正常构建和加载引擎。
